{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pattern-centric Fossil Recognition (Geo Fossils-I)\n",
    "- Preprocess + mask fossils, compute classical texture features (154-dim)\n",
    "- Train ResNet-50 + ViT (ImageNet init), collect embeddings\n",
    "- Hybrid classifier (texture + deep)\n",
    "- XAI: Grad-CAM, occlusion; TCAV optional if concept patches provided\n",
    "\n",
    "**Result figures produced** (saved under `figures/`):\n",
    "- `cm_cnn.png`, `cm_vit.png`: confusion matrices\n",
    "- `hybrid_f1.png`: macro-F1 comparison (texture/deep/hybrid)\n",
    "- `gradcam_cnn.png`: Grad-CAM overlay highlighting texture cues\n",
    "- `occlusion_cnn.png`: occlusion sensitivity heatmap\n",
    "- `tcav_cnn.png` (optional): concept influence if `concept_bank/<concept>` patches exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Optional, Tuple\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "import timm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as T\n",
    "from captum.attr import Occlusion\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.filters import gabor\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "plt.switch_backend(\"Agg\")  # headless plotting\n",
    "\n",
    "@dataclass\n",
    "class Config:\n",
    "    root: Path\n",
    "    img_size: int = 224\n",
    "    batch_size: int = 1024\n",
    "    num_workers: int = 0\n",
    "    max_epochs: int = 100\n",
    "    patience: int = 8\n",
    "    lr: float = 1e-4\n",
    "    weight_decay: float = 1e-4\n",
    "    seeds: Tuple[int, ...] = (13, 23, 33)\n",
    "    device: str = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    figdir: Path = Path(\"figures\")\n",
    "    ckptdir: Path = Path(\"checkpoints\")\n",
    "    concept_root: Path = Path(\"concept_bank\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed: int) -> None: # set random seeds for reproducibility\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "def ensure_dir(path: Path) -> None: # create directory if it doesn't exist\n",
    "    path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def list_images_by_class(root: Path) -> List[Dict]: # list image paths and labels\n",
    "    manifest = []\n",
    "    classes = sorted([d for d in root.iterdir() if d.is_dir()]) # get class directories\n",
    "    class_to_idx = {cls.name: i for i, cls in enumerate(classes)} # map class names to indices\n",
    "    for cls in classes:\n",
    "        for img_path in sorted(cls.glob(\"*.jpg\")):\n",
    "            manifest.append({\"path\": img_path, \"label\": class_to_idx[cls.name], \"classname\": cls.name}) # add image info to manifest\n",
    "    return manifest\n",
    "\n",
    "def stratified_split(manifest: List[Dict], train_ratio=0.7, val_ratio=0.15) -> Dict[str, List[Dict]]: # stratified train/val/test split\n",
    "    df = pd.DataFrame(manifest)\n",
    "    train_df, temp_df = train_test_split(df, test_size=1 - train_ratio, stratify=df[\"label\"], random_state=42) \n",
    "    rel_val = val_ratio / (1 - train_ratio)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=1 - rel_val, stratify=temp_df[\"label\"], random_state=99)\n",
    "    return {\"train\": train_df.to_dict(\"records\"), \"val\": val_df.to_dict(\"records\"), \"test\": test_df.to_dict(\"records\")}\n",
    "\n",
    "def resize_and_pad(img: np.ndarray, size: int = 224) -> np.ndarray: # resize and pad image to square\n",
    "    h, w = img.shape[:2]\n",
    "    scale = size / min(h, w)\n",
    "    new_h, new_w = int(round(h * scale)), int(round(w * scale))\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    pad_vert = max(size - new_h, 0)\n",
    "    pad_h1 = pad_vert // 2\n",
    "    pad_h2 = pad_vert - pad_h1\n",
    "    pad_horiz = max(size - new_w, 0)\n",
    "    pad_w1 = pad_horiz // 2\n",
    "    pad_w2 = pad_horiz - pad_w1\n",
    "    padded = cv2.copyMakeBorder(resized, pad_h1, pad_h2, pad_w1, pad_w2, cv2.BORDER_CONSTANT, value=0)\n",
    "    return padded\n",
    "\n",
    "def clean_mask(mask: np.ndarray, min_component: int = 50) -> np.ndarray:            # clean by removing small components\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)             # remove noise\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)            # close gaps\n",
    "    num, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)  # connected components\n",
    "    if num <= 1:\n",
    "        return mask\n",
    "    largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])                            # find largest component\n",
    "    clean = (labels == largest).astype(np.uint8)                                    # retain largest component\n",
    "    if stats[largest, cv2.CC_STAT_AREA] < min_component:                            # if too small, return empty mask\n",
    "        return np.zeros_like(mask, dtype=np.uint8)                                  # empty mask\n",
    "    return clean\n",
    "\n",
    "def compute_mask(gray: np.ndarray, provided_mask: Optional[np.ndarray] = None) -> np.ndarray: # compute binary mask\n",
    "    if provided_mask is not None:\n",
    "        base = provided_mask.astype(np.uint8)\n",
    "    else:\n",
    "        _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)# Otsu's thresholding\n",
    "        base = thresh\n",
    "    mask = clean_mask(base)                                                         # clean the mask\n",
    "    return mask\n",
    "\n",
    "def normalize_tensor(image: torch.Tensor) -> torch.Tensor:                          # normalize with ImageNet stats\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)                        # ImageNet mean\n",
    "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)                         # ImageNet std\n",
    "    return (image - mean) / std                                                     # normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classical texture features (154 dims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gabor_features(gray: np.ndarray, mask: np.ndarray) -> np.ndarray:\n",
    "    orientations = np.linspace(0, np.pi, 5, endpoint=False)\n",
    "    wavelengths = [2, 4, 8, 16]\n",
    "    feats = []\n",
    "    masked_gray = gray * (mask > 0)\n",
    "    for theta in orientations:\n",
    "        for wl in wavelengths:\n",
    "            real, imag = gabor(masked_gray, frequency=1.0 / wl, theta=theta)\n",
    "            mag = np.sqrt(real**2 + imag**2)\n",
    "            resp = mag[mask > 0].mean() if mask.sum() > 0 else mag.mean()\n",
    "            feats.append(resp)\n",
    "    return np.array(feats, dtype=np.float32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FossilDataset(Dataset): # custom dataset for fossil images\n",
    "    def __init__(self, records: List[Dict], img_size: int = 224, augment: bool = False):\n",
    "        self.records = records; self.img_size = img_size; self.augment = augment\n",
    "        aug_transforms = [T.RandomHorizontalFlip(), T.RandomApply([T.RandomRotation(10)], p=0.5), T.RandomResizedCrop(img_size, scale=(0.9, 1.0), ratio=(0.9, 1.1)), T.ColorJitter(brightness=0.1, contrast=0.1)]\n",
    "        self.train_tf = T.Compose([T.Resize((img_size, img_size))] + aug_transforms + [T.ToTensor()])\n",
    "        self.eval_tf = T.Compose([T.Resize((img_size, img_size)), T.ToTensor()])\n",
    "    def __len__(self): return len(self.records)\n",
    "    def __getitem__(self, idx: int) -> Dict:\n",
    "        rec = self.records[idx]\n",
    "        img = cv2.cvtColor(cv2.imread(str(rec[\"path\"])), cv2.COLOR_BGR2RGB)\n",
    "        img = resize_and_pad(img, self.img_size)\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "        mask = compute_mask(gray)\n",
    "        if mask.sum() < 0.1 * mask.size:\n",
    "            ys, xs = np.nonzero(mask)\n",
    "            if len(xs) > 0:\n",
    "                x1, x2, y1, y2 = xs.min(), xs.max(), ys.min(), ys.max(); mask[y1:y2, x1:x2] = 1\n",
    "        texture = compute_texture_features(gray, mask)\n",
    "        pil_img = to_pil_image(img)\n",
    "        tensor = self.train_tf(pil_img) if self.augment else self.eval_tf(pil_img)\n",
    "        tensor = normalize_tensor(tensor)\n",
    "        return {\"image\": tensor, \"label\": rec[\"label\"], \"texture\": torch.tensor(texture, dtype=torch.float32), \"mask\": torch.tensor(mask, dtype=torch.uint8), \"path\": str(rec[\"path\"])}\n",
    "\n",
    "def make_loaders(splits: Dict[str, List[Dict]], cfg: Config) -> Dict[str, DataLoader]:\n",
    "    loaders = {}\n",
    "    for split, augment in zip([\"train\", \"val\", \"test\"], [True, False, False]):\n",
    "        ds = FossilDataset(splits[split], img_size=cfg.img_size, augment=augment)\n",
    "        loaders[split] = DataLoader(ds, batch_size=cfg.batch_size, shuffle=augment, num_workers=cfg.num_workers, pin_memory=True)\n",
    "    return loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNetClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__(); self.backbone = torch.hub.load(\"pytorch/vision:v0.15.2\", \"resnet50\", weights=\"IMAGENET1K_V2\")\n",
    "        feat_dim = self.backbone.fc.in_features; self.backbone.fc = nn.Identity(); self.classifier = nn.Linear(feat_dim, num_classes)\n",
    "    def forward(self, x): feats = self.backbone(x); logits = self.classifier(feats); return logits, feats\n",
    "\n",
    "class ViTClassifier(nn.Module):\n",
    "    def __init__(self, num_classes: int):\n",
    "        super().__init__(); self.backbone = timm.create_model(\"vit_small_patch16_224\", pretrained=True, num_classes=0, global_pool=\"token\")\n",
    "        feat_dim = self.backbone.num_features; self.classifier = nn.Linear(feat_dim, num_classes)\n",
    "    def forward(self, x): feats = self.backbone(x); logits = self.classifier(feats); return logits, feats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_epoch(model: nn.Module, loader: DataLoader, optimizer, device: str, train: bool = True, split: str = \"train\"):\n",
    "    model.train(train)\n",
    "    total_loss = 0.0\n",
    "    all_preds, all_labels = [], []\n",
    "    iterator = tqdm(loader, desc=f\"{split} ({'train' if train else 'eval'})\", leave=False)\n",
    "    for batch in iterator:\n",
    "        imgs = batch[\"image\"].to(device)\n",
    "        labels = batch[\"label\"].to(device)\n",
    "        logits, _ = model(imgs)\n",
    "        loss = F.cross_entropy(logits, labels)\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = logits.argmax(dim=1).detach().cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(labels.cpu().numpy())\n",
    "        iterator.set_postfix({\"loss\": f\"{loss.item():.4f}\"})\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    return avg_loss, acc, f1\n",
    "\n",
    "def train_model(model: nn.Module, loaders: Dict[str, DataLoader], cfg: Config, tag: str) -> nn.Module:\n",
    "    device = cfg.device\n",
    "    model.to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=cfg.lr, weight_decay=cfg.weight_decay)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=cfg.max_epochs)\n",
    "    best_f1 = -1\n",
    "    best_state = None\n",
    "    wait = 0\n",
    "    for epoch in range(cfg.max_epochs):\n",
    "        train_loss, train_acc, train_f1 = run_epoch(model, loaders[\"train\"], optimizer, device, train=True, split=f\"train {epoch+1}/{cfg.max_epochs}\")\n",
    "        val_loss, val_acc, val_f1 = run_epoch(model, loaders[\"val\"], optimizer, device, train=False, split=f\"val {epoch+1}/{cfg.max_epochs}\")\n",
    "        scheduler.step()\n",
    "        print(f\"[{tag}] Epoch {epoch+1:03d} | train f1 {train_f1:.3f} acc {train_acc:.3f} | val f1 {val_f1:.3f} acc {val_acc:.3f}\")\n",
    "        if val_f1 > best_f1:\n",
    "            best_f1 = val_f1\n",
    "            best_state = model.state_dict()\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait >= cfg.patience:\n",
    "            print(f\"[{tag}] Early stop at epoch {epoch+1}\")\n",
    "            break\n",
    "    if best_state is None:\n",
    "        best_state = model.state_dict()\n",
    "    model.load_state_dict(best_state)\n",
    "    ensure_dir(cfg.ckptdir)\n",
    "    torch.save(model.state_dict(), cfg.ckptdir / f\"{tag}_best.pth\")\n",
    "    return model\n",
    "\n",
    "def evaluate(model: nn.Module, loader: DataLoader, device: str) -> Dict:\n",
    "    model.eval()\n",
    "    preds, labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            logits, _ = model(batch[\"image\"].to(device))\n",
    "            preds.append(logits.argmax(dim=1).cpu().numpy())\n",
    "            labels.append(batch[\"label\"].cpu().numpy())\n",
    "    preds = np.concatenate(preds)\n",
    "    labels = np.concatenate(labels)\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    return {\n",
    "        \"acc\": accuracy_score(labels, preds),\n",
    "        \"f1\": f1_score(labels, preds, average=\"macro\"),\n",
    "        \"cm\": cm,\n",
    "        \"labels\": labels,\n",
    "        \"preds\": preds,\n",
    "    }\n",
    "\n",
    "def plot_confusion(cm: np.ndarray, classes: List[str], figpath: Path, title: str):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figpath, dpi=200)\n",
    "    plt.close(fig)\n",
    "\n",
    "def plot_metric_bar(results: Dict[str, Dict], figpath: Path, metric: str = \"f1\"):\n",
    "    names = list(results.keys())\n",
    "    vals = [results[n][metric] for n in names]\n",
    "    fig, ax = plt.subplots(figsize=(6, 4))\n",
    "    sns.barplot(x=names, y=vals, ax=ax)\n",
    "    ax.set_ylabel(metric)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_title(f\"{metric} comparison\")\n",
    "    fig.tight_layout()\n",
    "    fig.savefig(figpath, dpi=200)\n",
    "    plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hybrid classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def collect_embeddings(model, loader, device: str): # collect deep features, texture features, labels\n",
    "    model.eval(); all_feats, all_textures, all_labels = [], [], []\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"image\"].to(device)\n",
    "        logits, feats = model(imgs)\n",
    "        all_feats.append(feats.cpu().numpy())\n",
    "        all_textures.append(batch[\"texture\"].numpy())\n",
    "        all_labels.append(batch[\"label\"].numpy())\n",
    "    return np.concatenate(all_feats), np.concatenate(all_textures), np.concatenate(all_labels)\n",
    "\n",
    "def train_hybrid(feats, textures, labels):\n",
    "    X_tex = textures; X_feat = feats; X_hybrid = np.concatenate([textures, feats], axis=1)\n",
    "    models = {\n",
    "        \"texture_only\": LogisticRegression(max_iter=200, n_jobs=-1, multi_class=\"auto\").fit(X_tex, labels),\n",
    "        \"deep_only\": LogisticRegression(max_iter=200, n_jobs=-1, multi_class=\"auto\").fit(X_feat, labels),\n",
    "        \"hybrid_logreg\": LogisticRegression(max_iter=200, n_jobs=-1, multi_class=\"auto\").fit(X_hybrid, labels),\n",
    "        \"hybrid_mlp\": MLPClassifier(hidden_layer_sizes=(256,), max_iter=300).fit(X_hybrid, labels),\n",
    "    }\n",
    "    return models\n",
    "\n",
    "def eval_hybrid(models, feats, textures, labels):\n",
    "    X_tex = textures; X_feat = feats; X_hybrid = np.concatenate([textures, feats], axis=1)\n",
    "    results = {}\n",
    "    for name, model in models.items():\n",
    "        if \"tex\" in name:\n",
    "            X = X_tex\n",
    "        elif \"deep\" in name and \"hybrid\" not in name:\n",
    "            X = X_feat\n",
    "        else:\n",
    "            X = X_hybrid\n",
    "        preds = model.predict(X)\n",
    "        results[name] = {\"acc\": accuracy_score(labels, preds), \"f1\": f1_score(labels, preds, average=\"macro\")}\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# XAI\n",
    "def grad_cam_cnn(model: ResNetClassifier, img_tensor: torch.Tensor, target_class: int) -> np.ndarray:\n",
    "    model.eval(); target_layer = model.backbone.layer4[-1].conv3; activations, gradients = [], []\n",
    "    def fwd_hook(_, __, output): activations.append(output.detach())\n",
    "    def bwd_hook(_, grad_input, grad_output): gradients.append(grad_output[0].detach())\n",
    "    handle_fwd = target_layer.register_forward_hook(fwd_hook); handle_bwd = target_layer.register_full_backward_hook(bwd_hook)\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(next(model.parameters()).device); logits, _ = model(img_tensor); score = logits[0, target_class]\n",
    "    model.zero_grad(); score.backward(); acts = activations[0]; grads = gradients[0]; weights = grads.mean(dim=(2, 3), keepdim=True)\n",
    "    cam = (weights * acts).sum(dim=1); cam = F.relu(cam); cam = cam.squeeze().cpu().numpy(); cam = (cam - cam.min()) / (cam.max() - cam.min() + 1e-6)\n",
    "    handle_fwd.remove(); handle_bwd.remove(); return cam\n",
    "\n",
    "def occlusion_sensitivity(model, img_tensor: torch.Tensor, target_class: int, patch: int = 32) -> np.ndarray:\n",
    "    model.eval(); device = next(model.parameters()).device; img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "    def forward_fn(x): logits, _ = model(x); return logits\n",
    "    occ = Occlusion(forward_fn)\n",
    "    attributions = occ.attribute(img_tensor, strides=(3, patch, patch), sliding_window_shapes=(3, patch, patch), target=target_class)\n",
    "    attr = attributions.squeeze().mean(dim=0).cpu().numpy(); attr = (attr - attr.min()) / (attr.max() - attr.min() + 1e-8); return attr\n",
    "\n",
    "def overlay_cam(img: np.ndarray, cam: np.ndarray, alpha: float = 0.4) -> np.ndarray:\n",
    "    cam_resized = cv2.resize(cam, (img.shape[1], img.shape[0])); heatmap = cv2.applyColorMap(np.uint8(255 * cam_resized), cv2.COLORMAP_JET)\n",
    "    overlay = cv2.addWeighted(img, 1 - alpha, heatmap, alpha, 0); return overlay\n",
    "\n",
    "def build_concept_directions(concept_root: Path, model: nn.Module, cfg: Config, random_pool: np.ndarray) -> Dict[str, np.ndarray]:\n",
    "    if not concept_root.exists():\n",
    "        print(f\"No concept bank at {concept_root}, skipping TCAV.\"); return {}\n",
    "    device = cfg.device; model.eval(); transform = T.Compose([T.Resize((cfg.img_size, cfg.img_size)), T.ToTensor(), normalize_tensor]); directions = {}\n",
    "    for concept_dir in concept_root.iterdir():\n",
    "        if not concept_dir.is_dir():\n",
    "            continue\n",
    "        feat_list = []\n",
    "        for img_path in concept_dir.glob(\"*\"):\n",
    "            if img_path.suffix.lower() not in [\".jpg\", \".png\", \".jpeg\"]:\n",
    "                continue\n",
    "            img = cv2.cvtColor(cv2.imread(str(img_path)), cv2.COLOR_BGR2RGB); img = resize_and_pad(img, cfg.img_size)\n",
    "            tensor = transform(to_pil_image(img)).unsqueeze(0).to(device)\n",
    "            with torch.no_grad():\n",
    "                _, feats = model(tensor)\n",
    "            feat_list.append(feats.cpu().numpy().squeeze())\n",
    "        if not feat_list:\n",
    "            continue\n",
    "        feats = np.stack(feat_list)\n",
    "        rand_idx = np.random.choice(len(random_pool), size=len(feats), replace=len(random_pool) < len(feats))\n",
    "        rand_feats = random_pool[rand_idx]\n",
    "        X = np.vstack([feats, rand_feats]); y = np.array([1] * len(feats) + [0] * len(rand_feats))\n",
    "        clf = LogisticRegression(max_iter=200).fit(X, y); direction = clf.coef_.flatten(); direction = direction / (np.linalg.norm(direction) + 1e-8)\n",
    "        directions[concept_dir.name] = direction\n",
    "    return directions\n",
    "\n",
    "def tcav_scores(model: nn.Module, loader: DataLoader, concept_dirs: Dict[str, np.ndarray], device: str) -> Dict[str, float]:\n",
    "    if not concept_dirs:\n",
    "        return {}\n",
    "    model.eval(); scores = {k: [] for k in concept_dirs}\n",
    "    for batch in loader:\n",
    "        imgs = batch[\"image\"].to(device); imgs.requires_grad_(True); logits, feats = model(imgs); preds = logits.argmax(dim=1)\n",
    "        for i in range(imgs.size(0)):\n",
    "            model.zero_grad(); logit = logits[i, preds[i]]; grad = torch.autograd.grad(logit, feats, retain_graph=True)[0][i]\n",
    "            for concept, direction in concept_dirs.items():\n",
    "                dir_t = torch.tensor(direction, device=device, dtype=grad.dtype)\n",
    "                directional = torch.dot(grad, dir_t)\n",
    "                scores[concept].append(float(directional.item() > 0))\n",
    "    return {k: float(np.mean(v)) if v else 0.0 for k, v in scores.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\s.swapnil/.cache\\torch\\hub\\pytorch_vision_v0.15.2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9238c7408af7449a80515db70e8c5276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train 1/100 (train):   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Orchestrate full run on Geo Fossils-I (runtime depends on hardware)\n",
    "cfg = Config(root=Path(\"geo fossil I\"), num_workers=0)\n",
    "print(\"Using device:\", cfg.device)\n",
    "ensure_dir(cfg.figdir); ensure_dir(cfg.ckptdir)\n",
    "manifest = list_images_by_class(cfg.root); classnames = sorted({r[\"classname\"] for r in manifest}); splits = stratified_split(manifest)\n",
    "with open(\"splits.json\", \"w\") as f:\n",
    "    json.dump({k: [str(r[\"path\"]) for r in v] for k, v in splits.items()}, f, indent=2)\n",
    "loaders = make_loaders(splits, cfg)\n",
    "\n",
    "set_seed(cfg.seeds[0]); cnn = ResNetClassifier(num_classes=len(classnames)); cnn = train_model(cnn, loaders, cfg, tag=\"cnn\")\n",
    "cnn_test = evaluate(cnn, loaders[\"test\"], cfg.device); plot_confusion(cnn_test[\"cm\"], classnames, cfg.figdir / \"cm_cnn.png\", \"CNN Confusion (Geo Fossils-I)\")\n",
    "\n",
    "set_seed(cfg.seeds[1]); vit = ViTClassifier(num_classes=len(classnames)); vit = train_model(vit, loaders, cfg, tag=\"vit\")\n",
    "vit_test = evaluate(vit, loaders[\"test\"], cfg.device); plot_confusion(vit_test[\"cm\"], classnames, cfg.figdir / \"cm_vit.png\", \"ViT Confusion (Geo Fossils-I)\")\n",
    "\n",
    "feats_train, tex_train, y_train = collect_embeddings(cnn, loaders[\"train\"], cfg.device)\n",
    "feats_test, tex_test, y_test = collect_embeddings(cnn, loaders[\"test\"], cfg.device)\n",
    "hybrid_models = train_hybrid(feats_train, tex_train, y_train); hybrid_results = eval_hybrid(hybrid_models, feats_test, tex_test, y_test)\n",
    "hybrid_results[\"cnn_only\"] = {\"acc\": cnn_test[\"acc\"], \"f1\": cnn_test[\"f1\"]}; plot_metric_bar(hybrid_results, cfg.figdir / \"hybrid_f1.png\", metric=\"f1\")\n",
    "\n",
    "concept_dirs = build_concept_directions(cfg.concept_root, cnn, cfg, feats_train)\n",
    "if concept_dirs:\n",
    "    tcav = tcav_scores(cnn, loaders[\"test\"], concept_dirs, cfg.device)\n",
    "    if tcav:\n",
    "        fig, ax = plt.subplots(figsize=(5, 3)); names = list(tcav.keys()); vals = [tcav[k] for k in names]\n",
    "        sns.barplot(x=names, y=vals, ax=ax); ax.set_ylabel(\"TCAV score (fraction positive)\"); ax.set_title(\"Concept influence (CNN)\")\n",
    "        fig.tight_layout(); fig.savefig(cfg.figdir / \"tcav_cnn.png\", dpi=200); plt.close(fig)\n",
    "\n",
    "batch = next(iter(loaders[\"test\"])); img_tensor = batch[\"image\"][0]\n",
    "img_np = (img_tensor.permute(1, 2, 0).numpy() * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])); img_np = np.clip(img_np * 255, 0, 255).astype(np.uint8)\n",
    "cnn_logits, _ = cnn(img_tensor.unsqueeze(0).to(cfg.device)); pred_class = int(cnn_logits.argmax(dim=1).item())\n",
    "cam = grad_cam_cnn(cnn, img_tensor, pred_class); cam_overlay = overlay_cam(img_np, cam); plt.imsave(cfg.figdir / \"gradcam_cnn.png\", cam_overlay)\n",
    "occ = occlusion_sensitivity(cnn, img_tensor, pred_class, patch=28); plt.imsave(cfg.figdir / \"occlusion_cnn.png\", occ, cmap=\"inferno\")\n",
    "\n",
    "print(\"Saved figures:\", list(cfg.figdir.glob(\"*.png\")))\n",
    "print(\"CNN test macro-F1:\", cnn_test[\"f1\"], \"ViT test macro-F1:\", vit_test[\"f1\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use the figures in your Results section\n",
    "- `cm_cnn.png` / `cm_vit.png`: report per-class confusions and overall macro-F1; compare CNN vs ViT.\n",
    "- `hybrid_f1.png`: shows gains from adding interpretable texture features (texture-only, deep-only, hybrid).\n",
    "- `gradcam_cnn.png`: highlight salient texture motifs (spirals, ridges, reticulation) the CNN uses.\n",
    "- `occlusion_cnn.png`: evidences critical patches?dark areas = drop in confidence when occluded.\n",
    "- `tcav_cnn.png` (if available): concept influence scores quantifying reliance on spiral/ridge/etc. patches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduced-FID (reduced-FID/): Multiple real fossil classes (e.g., ammonoid, agnatha, amphibian, angiosperm, avialae, etc.).\n",
    "\n",
    "- Test: Zero-shot classification with your trained CNN/ViT/hybrid. Build a fixed eval split per class; report accuracy/macro-F1 and confusion matrices (e.g., cm_fid.png).\n",
    "- XAI: Run Grad-CAM/occlusion on a few images per class to see which real-image textures are used.\n",
    "- Texture drift: Compute the 154-dim texture vector on salient regions and compare distributions vs Geo Fossils-I (ridge spacing, GLCM contrast, roughness).\n",
    "- Concept reliance: If you have a concept bank, run TCAV on FID to see if spiral/ridge/reticulate concepts still drive decisions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
