{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow pattern-centric fossil pipeline (Geo Fossils-I)\n",
    "- Preprocess + mask fossils, compute classical texture features (154-dim)\n",
    "- TF model: EfficientNetB0 with concatenated texture vector\n",
    "- Train/val/test on Geo Fossils-I; confusion matrix + Grad-CAM\n",
    "\n",
    "Figures saved to `figures_tf/`:\n",
    "- `cm_tf.png`: confusion matrix\n",
    "- `gradcam_tf.png`: Grad-CAM overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "DLL load failed while importing cv2: The specified module could not be found.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Dict, List\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcv2\u001b[39;00m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmatplotlib\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mplt\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mnp\u001b[39;00m\n",
      "\u001b[31mImportError\u001b[39m: DLL load failed while importing cv2: The specified module could not be found."
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Dict, List\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, accuracy_score, confusion_matrix, f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from skimage.feature import graycomatrix, graycoprops, local_binary_pattern\n",
    "from skimage.filters import gabor\n",
    "\n",
    "tf.get_logger().setLevel(\"ERROR\")\n",
    "print(\"TensorFlow version:\", tf.__version__)\n",
    "\n",
    "class Config:\n",
    "    def __init__(self, root: Path, img_size: int = 224, batch_size: int = 32, epochs: int = 25):\n",
    "        self.root = Path(root)\n",
    "        self.img_size = img_size\n",
    "        self.batch_size = batch_size\n",
    "        self.epochs = epochs\n",
    "        self.figdir = Path(\"figures_tf\")\n",
    "        self.figdir.mkdir(parents=True, exist_ok=True)\n",
    "        self.device = \"cuda\" if tf.config.list_physical_devices(\"GPU\") else \"cpu\"\n",
    "        print(\"Using device:\", self.device)\n",
    "\n",
    "cfg = Config(root=Path(\"geo fossil I\"), batch_size=32, epochs=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "def set_seed(seed: int = 13):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "\n",
    "def list_images_by_class(root: Path):\n",
    "    manifest = []\n",
    "    classes = sorted([d for d in root.iterdir() if d.is_dir()])\n",
    "    class_to_idx = {cls.name: i for i, cls in enumerate(classes)}\n",
    "    for cls in classes:\n",
    "        for img_path in sorted(cls.glob(\"*.jpg\")):\n",
    "            manifest.append({\"path\": str(img_path), \"label\": class_to_idx[cls.name], \"classname\": cls.name})\n",
    "    return manifest, classes\n",
    "\n",
    "def stratified_split(manifest: List[Dict], train_ratio=0.7, val_ratio=0.15):\n",
    "    df = pd.DataFrame(manifest)\n",
    "    train_df, temp_df = train_test_split(df, test_size=1 - train_ratio, stratify=df[\"label\"], random_state=42)\n",
    "    rel_val = val_ratio / (1 - train_ratio)\n",
    "    val_df, test_df = train_test_split(temp_df, test_size=1 - rel_val, stratify=temp_df[\"label\"], random_state=99)\n",
    "    return train_df.to_dict(\"records\"), val_df.to_dict(\"records\"), test_df.to_dict(\"records\")\n",
    "\n",
    "def resize_and_pad(img: np.ndarray, size: int = 224) -> np.ndarray:\n",
    "    h, w = img.shape[:2]\n",
    "    scale = size / min(h, w)\n",
    "    new_h, new_w = int(round(h * scale)), int(round(w * scale))\n",
    "    resized = cv2.resize(img, (new_w, new_h), interpolation=cv2.INTER_AREA)\n",
    "    pad_vert = max(size - new_h, 0)\n",
    "    pad_h1 = pad_vert // 2; pad_h2 = pad_vert - pad_h1\n",
    "    pad_horiz = max(size - new_w, 0)\n",
    "    pad_w1 = pad_horiz // 2; pad_w2 = pad_horiz - pad_w1\n",
    "    padded = cv2.copyMakeBorder(resized, pad_h1, pad_h2, pad_w1, pad_w2, cv2.BORDER_CONSTANT, value=0)\n",
    "    return padded\n",
    "\n",
    "def clean_mask(mask: np.ndarray, min_component: int = 50) -> np.ndarray:\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    kernel = np.ones((3, 3), np.uint8)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_OPEN, kernel, iterations=1)\n",
    "    mask = cv2.morphologyEx(mask, cv2.MORPH_CLOSE, kernel, iterations=2)\n",
    "    num, labels, stats, _ = cv2.connectedComponentsWithStats(mask, connectivity=8)\n",
    "    if num <= 1:\n",
    "        return mask\n",
    "    largest = 1 + np.argmax(stats[1:, cv2.CC_STAT_AREA])\n",
    "    clean = (labels == largest).astype(np.uint8)\n",
    "    if stats[largest, cv2.CC_STAT_AREA] < min_component:\n",
    "        return np.zeros_like(mask, dtype=np.uint8)\n",
    "    return clean\n",
    "\n",
    "def compute_mask(gray: np.ndarray) -> np.ndarray:\n",
    "    _, thresh = cv2.threshold(gray, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "    mask = clean_mask(thresh)\n",
    "    return mask\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Texture features (154 dims)\n",
    "def edge_features(gray, mask):\n",
    "    edges = cv2.Canny(gray, 100, 200) * mask\n",
    "    coords = np.column_stack(np.nonzero(edges))\n",
    "    if len(coords) == 0:\n",
    "        return np.zeros(20, dtype=np.float32)\n",
    "    sobelx = cv2.Sobel(gray, cv2.CV_64F, 1, 0, ksize=3)\n",
    "    sobely = cv2.Sobel(gray, cv2.CV_64F, 0, 1, ksize=3)\n",
    "    angles = (np.arctan2(sobely, sobelx) + np.pi) * (180.0 / np.pi)\n",
    "    ang_masked = angles[edges > 0]\n",
    "    hist, _ = np.histogram(ang_masked, bins=18, range=(0, 180))\n",
    "    edge_density = edges.sum() / mask.size\n",
    "    mean_length = len(coords) / (np.count_nonzero(hist) + 1e-6)\n",
    "    return np.concatenate(([edge_density, mean_length], hist.astype(np.float32)))\n",
    "\n",
    "def glcm_features(gray, mask):\n",
    "    masked = gray.copy(); masked[mask == 0] = 0\n",
    "    levels = 32\n",
    "    quant = (masked / (256 / levels)).astype(np.uint8)\n",
    "    distances = [1, 2]; angles = [0, np.pi / 4, np.pi / 2, 3 * np.pi / 4]\n",
    "    glcm = graycomatrix(quant, distances=distances, angles=angles, levels=levels, symmetric=True, normed=True)\n",
    "    feats = []\n",
    "    for prop in (\"contrast\", \"homogeneity\", \"energy\", \"correlation\"):\n",
    "        vals = graycoprops(glcm, prop).flatten(); feats.append(vals)\n",
    "    return np.concatenate(feats).astype(np.float32)\n",
    "\n",
    "def lbp_features(gray, mask):\n",
    "    lbp = local_binary_pattern(gray, P=8, R=1, method=\"uniform\")\n",
    "    lbp_masked = lbp[mask > 0]\n",
    "    hist, _ = np.histogram(lbp_masked, bins=np.arange(0, 60), range=(0, 59), density=True)\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "def fft_radial_energy(gray, mask, bands: int = 16):\n",
    "    f = np.fft.fft2(gray * mask)\n",
    "    fshift = np.fft.fftshift(f)\n",
    "    magnitude = np.abs(fshift)\n",
    "    h, w = magnitude.shape\n",
    "    y, x = np.indices((h, w))\n",
    "    center = np.array([(h - 1) / 2.0, (w - 1) / 2.0])\n",
    "    r = np.sqrt((x - center[1]) ** 2 + (y - center[0]) ** 2)\n",
    "    r_norm = r / r.max()\n",
    "    hist, _ = np.histogram(r_norm, bins=bands, range=(0.0, 1.0), weights=magnitude)\n",
    "    hist = hist / (hist.sum() + 1e-8)\n",
    "    return hist.astype(np.float32)\n",
    "\n",
    "def gabor_features(gray, mask):\n",
    "    orientations = np.linspace(0, np.pi, 5, endpoint=False)\n",
    "    wavelengths = [2, 4, 8, 16]\n",
    "    feats = []\n",
    "    masked_gray = gray * (mask > 0)\n",
    "    for theta in orientations:\n",
    "        for wl in wavelengths:\n",
    "            real, imag = gabor(masked_gray, frequency=1.0 / wl, theta=theta)\n",
    "            mag = np.sqrt(real**2 + imag**2)\n",
    "            resp = mag[mask > 0].mean() if mask.sum() > 0 else mag.mean()\n",
    "            feats.append(resp)\n",
    "    return np.array(feats, dtype=np.float32)\n",
    "\n",
    "def spiral_curvature(mask):\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    if not contours:\n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    cnt = max(contours, key=cv2.contourArea)\n",
    "    if len(cnt) < 5:\n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    pts = cnt[:, 0, :].astype(np.float32)\n",
    "    dx = np.gradient(pts[:, 0]); dy = np.gradient(pts[:, 1]); ddx = np.gradient(dx); ddy = np.gradient(dy)\n",
    "    curvature = np.abs(dx * ddy - dy * ddx) / (np.power(dx**2 + dy**2, 1.5) + 1e-6)\n",
    "    curv_valid = curvature[np.isfinite(curvature)]\n",
    "    if len(curv_valid) == 0:\n",
    "        return np.zeros(4, dtype=np.float32)\n",
    "    mean_c = curv_valid.mean(); std_c = curv_valid.std(); p95_c = np.percentile(curv_valid, 95)\n",
    "    axes = (1.0, 1.0)\n",
    "    if len(cnt) >= 5:\n",
    "        ellipse = cv2.fitEllipse(cnt); axes = ellipse[1]\n",
    "    major = max(axes); minor = min(axes) + 1e-6; pitch_ratio = major / minor\n",
    "    return np.array([mean_c, std_c, p95_c, pitch_ratio], dtype=np.float32)\n",
    "\n",
    "def shape_features(mask):\n",
    "    area = float(mask.sum())\n",
    "    contours, _ = cv2.findContours(mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    perimeter = float(cv2.arcLength(contours[0], True)) if contours else 0.0\n",
    "    roundness = 4 * math.pi * area / (perimeter**2 + 1e-6) if perimeter > 0 else 0.0\n",
    "    return np.array([area, perimeter, roundness], dtype=np.float32)\n",
    "\n",
    "def compute_texture_features(gray, mask):\n",
    "    mask = (mask > 0).astype(np.uint8)\n",
    "    if mask.sum() == 0:\n",
    "        return np.zeros(154, dtype=np.float32)\n",
    "    feats = [\n",
    "        edge_features(gray, mask),\n",
    "        glcm_features(gray, mask),\n",
    "        lbp_features(gray, mask),\n",
    "        fft_radial_energy(gray, mask),\n",
    "        gabor_features(gray, mask),\n",
    "        spiral_curvature(mask),\n",
    "        shape_features(mask),\n",
    "    ]\n",
    "    return np.concatenate(feats).astype(np.float32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf.data preprocessing wrapper\n",
    "def preprocess_np(path, label, img_size):\n",
    "    # path comes from tf.numpy_function as a NumPy scalar (bytes)\n",
    "    # make it a normal Python string\n",
    "    if isinstance(path, np.ndarray):\n",
    "        path = path.item()          # get scalar\n",
    "    path = path.decode(\"utf-8\")     # bytes -> str\n",
    "\n",
    "    # img_size is a NumPy scalar, ensure it's a Python int\n",
    "    if isinstance(img_size, np.ndarray):\n",
    "        img_size = int(img_size.item())\n",
    "    else:\n",
    "        img_size = int(img_size)\n",
    "\n",
    "    # --- your existing logic ---\n",
    "    img = cv2.cvtColor(cv2.imread(path), cv2.COLOR_BGR2RGB)\n",
    "    img = resize_and_pad(img, img_size)\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "    mask = compute_mask(gray)\n",
    "    texture = compute_texture_features(gray, mask)\n",
    "\n",
    "    # normalize image\n",
    "    img = img.astype(np.float32) / 255.0\n",
    "    img = (img - np.array([0.485, 0.456, 0.406], dtype=np.float32)) / \\\n",
    "          np.array([0.229, 0.224, 0.225], dtype=np.float32)\n",
    "\n",
    "    # ensure texture is float32 to match Tout=tf.float32\n",
    "    texture = texture.astype(np.float32)\n",
    "\n",
    "    # label as int32 to match Tout=tf.int32\n",
    "    return img, np.int32(label), texture\n",
    "\n",
    "def tf_preprocess(img_path, label, img_size=224):\n",
    "    img, lbl, tex = tf.numpy_function(\n",
    "        func=preprocess_np,\n",
    "        inp=[img_path, label, tf.constant(img_size, dtype=tf.int64)],\n",
    "        Tout=[tf.float32, tf.int32, tf.float32],\n",
    "    )\n",
    "\n",
    "    # Static shapes for tf.data / model\n",
    "    img.set_shape((img_size, img_size, 3))\n",
    "    tex.set_shape((154,))   # as you specified\n",
    "    lbl.set_shape(())\n",
    "\n",
    "    return (img, tex), lbl\n",
    "\n",
    "def make_dataset(records, batch_size=32, shuffle=True, img_size=224):\n",
    "    paths = [r[\"path\"] for r in records]; labels = [r[\"label\"] for r in records]\n",
    "    ds = tf.data.Dataset.from_tensor_slices((paths, labels))\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(buffer_size=len(paths), seed=42, reshuffle_each_iteration=True)\n",
    "    ds = ds.map(lambda p, l: tf_preprocess(p, l, img_size), num_parallel_calls=tf.data.AUTOTUNE)\n",
    "    ds = ds.batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
    "    return ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and plotting\n",
    "def build_model(num_classes: int, use_texture: bool = True):\n",
    "    img_in = tf.keras.Input(shape=(cfg.img_size, cfg.img_size, 3), name=\"image\")\n",
    "    tex_in = tf.keras.Input(shape=(154,), name=\"texture\")\n",
    "    base = tf.keras.applications.EfficientNetB0(include_top=False, weights=\"imagenet\", pooling=\"avg\")\n",
    "    x = base(img_in)\n",
    "    if use_texture:\n",
    "        x = tf.keras.layers.Concatenate()([x, tex_in])\n",
    "    x = tf.keras.layers.Dense(256, activation=\"relu\")(x)\n",
    "    x = tf.keras.layers.Dropout(0.3)(x)\n",
    "    out = tf.keras.layers.Dense(num_classes, activation=\"softmax\")(x)\n",
    "    model = tf.keras.Model(inputs=[img_in, tex_in], outputs=out)\n",
    "    return model\n",
    "\n",
    "def plot_confusion(cm, classes, figpath: Path, title: str):\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=classes)\n",
    "    disp.plot(ax=ax, cmap=\"Blues\", colorbar=False)\n",
    "    ax.set_title(title)\n",
    "    fig.tight_layout(); fig.savefig(figpath, dpi=200); plt.close(fig)\n",
    "\n",
    "def grad_cam_tf(model, image, label_idx, layer_name=None):\n",
    "    if layer_name is None:\n",
    "        conv_layers = [l.name for l in model.layers if isinstance(l, tf.keras.layers.Conv2D)]\n",
    "        layer_name = conv_layers[-1]\n",
    "    grad_model = tf.keras.Model([model.inputs], [model.get_layer(layer_name).output, model.output])\n",
    "    with tf.GradientTape() as tape:\n",
    "        conv_out, preds = grad_model([tf.expand_dims(image[0], 0), tf.expand_dims(image[1], 0)])\n",
    "        loss = preds[:, label_idx]\n",
    "    grads = tape.gradient(loss, conv_out)[0]\n",
    "    pooled = tf.reduce_mean(grads, axis=(0, 1, 2))\n",
    "    conv_out = conv_out[0]\n",
    "    cam = tf.reduce_sum(tf.multiply(pooled, conv_out), axis=-1)\n",
    "    cam = tf.maximum(cam, 0)\n",
    "    cam = cam / (tf.reduce_max(cam) + 1e-8)\n",
    "    cam = tf.image.resize(cam[..., None], (cfg.img_size, cfg.img_size)).numpy().squeeze()\n",
    "    return cam\n",
    "\n",
    "def overlay_cam(img, cam, alpha=0.4):\n",
    "    cam_color = cv2.applyColorMap(np.uint8(255 * cam), cv2.COLORMAP_JET)\n",
    "    img_uint8 = np.clip((img * np.array([0.229, 0.224, 0.225]) + np.array([0.485, 0.456, 0.406])) * 255, 0, 255).astype(np.uint8)\n",
    "    overlay = cv2.addWeighted(img_uint8, 1 - alpha, cam_color, alpha, 0)\n",
    "    return overlay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run pipeline on Geo Fossils-I\n",
    "set_seed(13)\n",
    "manifest, classes = list_images_by_class(cfg.root)\n",
    "train_rec, val_rec, test_rec = stratified_split(manifest)\n",
    "with open(\"splits_tf.json\", \"w\") as f:\n",
    "    json.dump({k: [r[\"path\"] for r in v] for k, v in zip([\"train\", \"val\", \"test\"], [train_rec, val_rec, test_rec])}, f, indent=2)\n",
    "\n",
    "train_ds = make_dataset(train_rec, batch_size=cfg.batch_size, shuffle=True, img_size=cfg.img_size)\n",
    "val_ds = make_dataset(val_rec, batch_size=cfg.batch_size, shuffle=False, img_size=cfg.img_size)\n",
    "test_ds = make_dataset(test_rec, batch_size=cfg.batch_size, shuffle=False, img_size=cfg.img_size)\n",
    "\n",
    "model = build_model(num_classes=len(classes), use_texture=True)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-4), loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "es = tf.keras.callbacks.EarlyStopping(patience=5, restore_best_weights=True, monitor=\"val_accuracy\")\n",
    "history = model.fit(train_ds, validation_data=val_ds, epochs=cfg.epochs, callbacks=[es])\n",
    "\n",
    "# Evaluate\n",
    "y_true, y_pred = [], []\n",
    "for (imgs, tex), labels in test_ds:\n",
    "    preds = model.predict((imgs, tex), verbose=0)\n",
    "    y_true.extend(labels.numpy().tolist())\n",
    "    y_pred.extend(np.argmax(preds, axis=1).tolist())\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "plot_confusion(cm, [c.name for c in classes], cfg.figdir / \"cm_tf.png\", \"TF Confusion (Geo Fossils-I)\")\n",
    "print(\"Test accuracy:\", accuracy_score(y_true, y_pred), \"macro-F1:\", f1_score(y_true, y_pred, average=\"macro\"))\n",
    "\n",
    "# Grad-CAM on one test sample\n",
    "sample = next(iter(test_ds))\n",
    "img0 = sample[0][0][0].numpy(); tex0 = sample[0][1][0].numpy()\n",
    "pred_idx = int(np.argmax(model.predict((img0[None], tex0[None]), verbose=0)))\n",
    "cam = grad_cam_tf(model, (img0, tex0), label_idx=pred_idx)\n",
    "overlay = overlay_cam(img0, cam)\n",
    "plt.imsave(cfg.figdir / \"gradcam_tf.png\", overlay)\n",
    "print(\"Saved figures:\", list(cfg.figdir.glob(\"*.png\")))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results figures to cite\n",
    "- `cm_tf.png`: confusion matrix on Geo Fossils-I (TF pipeline)\n",
    "- `gradcam_tf.png`: Grad-CAM overlay showing salient texture cues"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
